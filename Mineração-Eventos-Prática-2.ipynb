{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mineração-Eventos-Prática-2.ipynb","version":"0.3.2","provenance":[{"file_id":"1MT8sjjTWa93-uqiv8vslF6XvcP5I7Q1E","timestamp":1567629977815}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Moq0yc0fT_im","colab_type":"code","outputId":"cbd23584-bb1c-43e6-c646-22209d8fa016","executionInfo":{"status":"ok","timestamp":1568281460940,"user_tz":180,"elapsed":86663,"user":{"displayName":"Ivan Filho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA-4ytfd2Jn2f8XkaH5P5LjntfLtlIkJgqhPI_i5Q=s64","userId":"04297106173447241424"}},"colab":{"base_uri":"https://localhost:8080/","height":746}},"source":["# Importando bibliotecas\n","import pandas as pd\n","import string\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('rslp')\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.porter import *\n","from scipy.spatial.distance import cosine\n","import numpy as np\n","import networkx as nx\n","!pip install polyglot\n","!pip install pyicu\n","!pip install pycld2\n","!pip install morfessor\n","from polyglot.downloader import downloader\n","!polyglot download embeddings2.pt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Unzipping stemmers/rslp.zip.\n","Collecting polyglot\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n","\u001b[K     |████████████████████████████████| 133kB 9.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: polyglot\n","  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52560 sha256=051890cd32b6e063e9f16f36a4f062fcfb5478c99a0febcfc973cb583d0d9871\n","  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n","Successfully built polyglot\n","Installing collected packages: polyglot\n","Successfully installed polyglot-16.7.4\n","Collecting pyicu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/35/211ffb949c68e688ade7d40426de030a24eaec4b6c45330eeb9c0285f43a/PyICU-2.3.1.tar.gz (214kB)\n","\u001b[K     |████████████████████████████████| 215kB 9.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyicu\n","  Building wheel for pyicu (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyicu: filename=PyICU-2.3.1-cp36-cp36m-linux_x86_64.whl size=1173054 sha256=631a6cec3c637b8c3c0be354ac23a3b1dbaae405bbb612164f88381ecbc16829\n","  Stored in directory: /root/.cache/pip/wheels/3f/45/7e/ccee9f1fe52787595e92641b5645cdf2cb40096749b39b4422\n","Successfully built pyicu\n","Installing collected packages: pyicu\n","Successfully installed pyicu-2.3.1\n","Collecting pycld2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/77/8525fe5f147bf2819c7c9942c717c4a79b83f8003da1a3847759fb560909/pycld2-0.31.tar.gz (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 9.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: pycld2\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.31-cp36-cp36m-linux_x86_64.whl size=9825428 sha256=201ffe69a926e02ea900954084c4ceb432803e8c19387fda2843f675cf3b465c\n","  Stored in directory: /root/.cache/pip/wheels/7b/44/44/ec4c5e25e095f02aa0e63ef2bf0cc8badda5877330ffa5fbe4\n","Successfully built pycld2\n","Installing collected packages: pycld2\n","Successfully installed pycld2-0.31\n","Collecting morfessor\n","  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n","Installing collected packages: morfessor\n","Successfully installed morfessor-2.0.6\n","[polyglot_data] Downloading package embeddings2.pt to\n","[polyglot_data]     /root/polyglot_data...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-jtKmwqvU4Vd","colab_type":"code","outputId":"787641bc-ffe0-4da9-95ab-09151765d4e9","executionInfo":{"status":"ok","timestamp":1568282170201,"user_tz":180,"elapsed":4597,"user":{"displayName":"Ivan Filho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA-4ytfd2Jn2f8XkaH5P5LjntfLtlIkJgqhPI_i5Q=s64","userId":"04297106173447241424"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["from polyglot.mapping import Embedding\n","embeddings = Embedding.load(\"/root/polyglot_data/embeddings2/pt/embeddings_pkl.tar.bz2\")\n","\n","neighbors = embeddings.nearest_neighbors(\"marido\") #palavras vizinhas de 'presidente'\n","neighbors"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['namorado',\n"," 'tio',\n"," 'pai',\n"," 'esposo',\n"," 'avô',\n"," 'genro',\n"," 'garoto',\n"," 'amigo',\n"," 'noivo',\n"," 'mordomo']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"Ov-t-k-rVZS6","colab_type":"code","colab":{}},"source":["# acessando o word vector\n","embeddings['presidente']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IdZG0HdVUMY4","colab_type":"code","colab":{}},"source":["# remoção de pontuacao e stopwords\n","\n","def remove_stopwords(text,lang,domain_stopwords=[]):\n","  \n","  stop_words = nltk.corpus.stopwords.words(lang) # lang='portuguese' or lang='english'\n","  \n","  s = str(text).lower() # tudo para caixa baixa\n","  table = str.maketrans({key: None for key in string.punctuation})\n","  s = s.translate(table) # remove pontuacao\n","  tokens = word_tokenize(s) #obtem tokens\n","  v = [i for i in tokens if not i in stop_words and not i in domain_stopwords and not i.isdigit()] # remove stopwords\n","  s = \"\"\n","  for token in v:\n","    s += token+\" \"\n","  return s.strip()\n","\n","\n","# exemplos de uso\n","text = \"O estudante de Inteligência Artificial foi na livraria comprar  livros para estudar.\"\n","text2 = remove_stopwords(text, 'portuguese')\n","print('Antes: '+text)\n","print('Depois: '+text2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sfbDywx9UR1_","colab_type":"code","colab":{}},"source":["# computando dissimilaridade de cosseno\n","\n","def dis_cosine_embedding(text1, text2, lang='portuguese'):\n","  s1 = remove_stopwords(text1,lang)\n","  s2 = remove_stopwords(text2,lang)\n","  tokens1 = word_tokenize(s1)\n","  tokens2 = word_tokenize(s2)\n","  v1 = np.array([0.0]*64)\n","  v2 = np.array([0.0]*64)\n","  for token in tokens1:\n","    if token in embeddings:\n","      v1 += embeddings[token]\n","  for token in tokens2:\n","    if token in embeddings:\n","      v2 += embeddings[token]\n","  dcos = cosine(v1,v2)\n","  return dcos\n","\n","\n","# exemplo: dissimilaride entre o primeiro (id=0) e o segundo evento (id=1) do vsm-tfidf\n","text1 = \"O número de casos de infecção aumentou\"\n","text2 = \"Cresceu a quantidade de doentes\"\n","dis_cosine_embedding(text1, text2)"],"execution_count":0,"outputs":[]}]}